<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="cf-amazon-s3-connector">
  <title>Publishing Content to Amazon S3</title>
  <body>
    <p>The Amazon S3 connector facilitates publishing deliverable build outputs to the designated
      bucket. </p>
    <section id="section_krc_5pl_2bc">
      <title>Requirements</title>
      <p>To publish content to an Amazon S3 bucket, you must have the following:</p>
      <ul id="ul_lcs_srl_2bc">
        <li>An AWS account.</li>
        <li>An existing S3 bucket configured. For more details, refer to the AWS <xref
            href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html" format="html"
            scope="external">S3 Documentation</xref>.</li>
        <li>The Access Key ID and Secret Access Key associated with your AWS account. For more
          details, refer to the AWS <xref
            href="https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html"
            format="html" scope="external">Security Credentials Documentation</xref>.</li>
      </ul>
      <p>For additional information, consult the <xref href="https://docs.aws.amazon.com/"
          format="html" scope="external">AWS Documentation</xref>.</p>
    </section>
    <section id="section_wzb_hql_2bc">
      <title>Define the Connector</title>
      <p>To configure an Amazon S3 connector, the following fields must be provided:<dl
          id="dl_uc3_4vy_dbc">
          <dlentry>
            <dt>Connector Name</dt>
            <dd>The name of the connector.</dd>
          </dlentry>
          <dlentry>
            <dt>Region Name</dt>
            <dd>The AWS Region where your Amazon S3 bucket is hosted. When you create a bucket, you
              specify the AWS Region where you want Amazon S3 to create the bucket.</dd>
          </dlentry>
          <dlentry>
            <dt>Bucket Name</dt>
            <dd>The Amazon S3 container used for storing your files.</dd>
          </dlentry>
          <dlentry>
            <dt>Access Key ID</dt>
            <dd>A unique identifier associated with your AWS account or with an IAM user.</dd>
          </dlentry>
          <dlentry>
            <dt>Secret Access Key</dt>
            <dd>The secret key used together with Access Key ID for authentication.</dd>
          </dlentry>
        </dl></p>
    </section>
    <section id="section_t53_svy_dbc">
      <title>Select the Amazon S3 Connector in the Deliverables Page</title>
      <p>To publish the output of a deliverable to Amazon S3, follow these steps: <ol
          id="ol_lmg_bsl_2bc">
          <li>After configuring the Amazon S3 connector, navigate to the
              <uicontrol>Deliverables</uicontrol> page, then either edit an existing deliverable or
            create a new one.</li>
          <li>In the resulting configuration page, select the <uicontrol>Upload output using a
              publishing connector</uicontrol> option.</li>
          <li>Use the drop-down list below that option to select the Amazon S3 connector that you
            previously defined.</li>
          <li>Specify the <filepath>Destination Folder</filepath> where the build output will be
            published. Ensure that the designated folder already exists within the Amazon S3
              Bucket.<note id="note_zch_2sl_2bc">The output will be published as a zip archive in
              this folder. </note></li>
          <li>If you want to upload the deliverable's build in ZIP format, select the
              <uicontrol>Upload the deliverable's build output in ZIP format</uicontrol> option. By
            default, it uploads all the build files into the destination folder.</li>
          <li>Click <uicontrol>Save</uicontrol> at the bottom of the configuration page.</li>
          <li>Begin the Deliverable build process. Upon completion, the build output will be
            published to the configured Amazon S3 bucket as a zip archive within the folder you
            specified as the destination folder.</li>
        </ol></p>
    </section>
    <section id="section_krd_3sl_2bc">
      <title>(Optional) Lambda Function to Unzip Content</title>
      <p>The Amazon S3 connector will publish the deliverable output as a zip file to the
        destination folder specified when configuring the deliverable. To extract the file, one
        approach is to <xref
          href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/transforming-objects.html"
          format="html" scope="external">configure a Lambda Function on AWS</xref> to unzip the
        archive within your designated <filepath>Documentation Folder</filepath>.<note
          id="note_a2b_xvy_dbc">The <filepath>Documentation Folder</filepath> should be the folder
          that is publicly accessible to users.</note></p>
      <p><b><u>Prerequisite:</u></b></p>
      <p id="p_mps_nhg_p1c">You must have an Amazon role that has <i>S3 Full Access</i> and the <i>AWS
        Lambda Basic Execution Role</i> policies.</p>
      <p id="p_bmx_4kg_p1c">
        <image href="../img/role_policies.png"/></p>
      <p id="p_twb_53g_p1c"><b><u>Procedure:</u></b></p>
      <p>
        <ol id="ol_a2d_3bg_41c">
          <li>Go to the AWS Lambda and create a new function.<note id="note_c4z_lbg_41c">Your
            Lambda function and the bucket where the documentation is stored should be
            in the same region.</note></li>
          <li>
            <p id="p_c13_mkg_p1c">Give your function a name, select the latest version of Python
              runtime, and your already created role.</p>
            <p id="p_ern_mkg_p1c"><image href="../img/lambda_config.png" id="image_axh_hcg_41c"/></p>
          </li>
          <li>In the <b>Code</b> section of the function, paste the code into the
              <filepath>lambda_function.py</filepath>
              file.<codeblock id="codeblock_tds_xdg_41c" outputclass="language-python">import zipfile
import urllib.parse
from io import BytesIO
from mimetypes import guess_type
import boto3
import logging
              
s3 = boto3.client('s3')
logging.getLogger().setLevel(logging.INFO)
              
# Target path to unzip the archive.
TARGET_PATH = "target"
              
def lambda_handler(event, context):
  # Bucket's name.
  bucket = event['Records'][0]['s3']['bucket']['name']
  logging.info("Using bucket: " + bucket);
              
  # Zip file's name.
  zip_key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'])
  logging.info("File to unzip: " + zip_key);
  logging.info("Path to unzp the files: " + TARGET_PATH);
              
  try:
    # Get the zipfile from S3 request.
    obj = s3.get_object(Bucket=bucket, Key=zip_key)
    z = zipfile.ZipFile(BytesIO(obj['Body'].read()))
              
    # Extract and upload each file to the bucket.
    for filename in z.namelist():
      content_type = str(guess_type(filename, strict=False)[0])
      s3.upload_fileobj(
        Fileobj=z.open(filename),
        Bucket=bucket,
        Key=TARGET_PATH + filename,
        ExtraArgs={'ContentType': content_type}
      )
              
    # Delete the archive after extraction.
    s3.delete_object(Bucket=bucket, Key=zip_key)
  except Exception as error:
    print(f"An error occurred {error=}")</codeblock><note id="note_rpt_ydg_41c">If you
              want to specify a custom target path, then modify the <b>TARGET_PATH</b> variable. For
              example, if you want to unzip the archive into the <b>Documentation</b> folder, then
              set the variable to: <codeph>TARGET_PATH = "Documentation/</codeph>.</note></li>
          <li>Click the <b>Deploy</b> button to save your changes.</li>
          <li><p>Go to the <uicontrol>Configuration</uicontrol> tab of the Lambda function.</p>
            <p id="p_jn2_zjg_p1c"><image href="../img/configuration_tab.png" id="image_ctq_m2g_41c"/></p></li>
          <li id="li_bdl_1kg_p1c">Click the <b>Edit</b> button of the <b>General
            Configuration</b>.</li>
          <li>Set the timeout to 15 minutes 0 seconds.<note id="note_amd_p2g_41c">If you do
            not do that, your Lambda function may get timed out and the zip will not be
            unzipped/deleted.</note></li>
          <li>Set the <b>Memory</b> to somewhere between 1000-5000 MB.</li>
          <li>Set the <b>Ephemeral storage</b> to somewhere between 1000-5000 MB.<note
            id="note_dbx_cqq_3bc">If your output's size is bigger than allocated Memory and
            Ephemeral storage the lambda function may fail to execute.</note></li>
          <li>Click <b>Save</b> button.</li>
          <li>
            <p id="p_q2f_qkg_p1c">In the <b>Function overview</b> section, click the <b>Add
              Trigger</b> button.</p>
            <p id="p_zym_qkg_p1c"><image id="image_qss_x2g_41c" href="../img/add_trigger.png"/></p>
          </li>
          <li>As a <b>Source Trigger</b>, select <b>S3</b>.</li>
          <li>Select your desired bucket.</li>
          <li>Select <b>All object create events</b> for <b>Event Types</b>.</li>
          <li>
            <p id="p_e5n_rkg_p1c">In the <b>Suffix</b> field, type <b>.zip</b> so that the Lambda
              function will only trigger for <b>.zip</b> files.</p>
            <p id="p_pbd_rkg_p1c"><image href="../img/trigger_config.png" id="image_qfk_jfg_41c"/></p>
          </li>
          <li>Click <b>Add</b>.</li>
        </ol>
      </p>
      <p><b>Result:</b> The AWS Lambda Function should trigger every time a <b>.zip</b> file is
        uploaded, and it will unzip it and replace the files in the target directory.</p>
    </section>
    <section id="section_jwq_511_fbc">
      <title>(Optional) Lifecycle Rule to Delete Incomplete Uploads</title>
      <p>The Amazon S3 connector will use multi-part uploads for larger files to optimize
        performance. If the upload is interrupted before completion, uploaded parts may remain
        stored inside your Amazon S3 bucket. It is recommended to <xref
          href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"
          format="html" scope="external">configure a <filepath>Lifecycle Rule</filepath></xref> for
        your bucket to automatically delete incomplete multi-part uploads. </p>
    </section>
  </body>
</topic>
